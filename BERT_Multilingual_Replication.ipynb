{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URKn8K0HydhV"
   },
   "source": [
    "# New analysis for paper with Ernesto and Susan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y73xWUkpU1AC"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Import necessary Python libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNxmMnzoccfm",
    "outputId": "93f0f194-f921-4113-e3a9-5d4a12d567cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9Si6kIWcULv"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from matplotlib import ticker\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyFTLgQduSgA",
    "outputId": "a8780af9-bcfc-4a2d-8d68-51eb59eabd7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YiBKhjkuaRE",
    "outputId": "888370c9-b260-4a8b-c27e-f937fe5fbac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germany_test.csv   italy_test.csv   netherlands_test.csv   poland_test.csv\n",
      "germany_train.csv  italy_train.csv  netherlands_train.csv  poland_train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/train_test_splits_for_BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRBYFmDgsVSn"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Read in data, and split into Train-Val-Test samples**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1VwkQnpuvVX"
   },
   "outputs": [],
   "source": [
    "trainfiles = glob(\"/content/drive/MyDrive/train_test_splits_for_BERT/*train.csv\")\n",
    "testfiles = glob(\"/content/drive/MyDrive/train_test_splits_for_BERT/*test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qzH7NrCveIH",
    "outputId": "461894c1-f3d3-45cd-fc0a-8eabd0b83f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 6388 train examples, 2130 validation examples, and 2843 test examples.\n"
     ]
    }
   ],
   "source": [
    "def read_files(filenames):\n",
    "  df = pd.DataFrame()\n",
    "  for fn in filenames:\n",
    "    _df = pd.read_csv(fn, encoding='iso-8859-1')\n",
    "    _df['is_sports'] = _df['TOPIC']==4\n",
    "    _df = _df[['title_blurb', 'is_sports']]\n",
    "    df = pd.concat([df, _df])\n",
    "  df = df.sample(frac=1, random_state=1983)  # we need to shuffle b/c otherwise it's sorted by language\n",
    "  return list(df['title_blurb']), list(df['is_sports'])\n",
    "\n",
    "X_train_val, y_train_val  = read_files(trainfiles)\n",
    "X_test, y_test  = read_files(testfiles)\n",
    "\n",
    "# Split your training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"We have {len(X_train)} train examples, {len(X_val)} validation examples, and {len(X_test)} test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-34oep8LNKw"
   },
   "source": [
    "Here's an example of a training text and training label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcxvqUsdf5Sm",
    "outputId": "c81bb64f-e22d-4b09-b9d3-a23dc1f9f839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Gersdorf: Obrona sadów budzi podziw demokratycznego swiata \"\"\"Pragne podziekowac wszystkim, którym nie jest obojetny stan polskiego sadownictwa. (...) Dziekuje Wam wszystkim, Wasza postawa jest wazna i budzi szacunek nie tylko mój i sedziów, ale calego demokratycznego swiata\"\" - napisala w liscie zamieszczonym na stronach SN I Prezes Malgorzat...\"',\n",
       " False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFga4Ci_tKPk",
    "outputId": "9c8cdd4d-cdfe-4aac-8760-5ac3006921c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piWsW9ZeaP_D"
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "\n",
    "# We'll run our code on NVIDIA GPUs using the program management system.\n",
    "device_name = 'cuda'\n",
    "\n",
    "# We set the maximum number of tokens in each document to be 512, which is the maximum length for BERT models.\n",
    "max_length = 512\n",
    "\n",
    "# We define the directory where we'll save our trained model. You can choose any name for the directory.\n",
    "save_directory = '/content/drive/MyDrive/my_trained_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FoaXKbKjXRX"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Implementing a Baseline Model using Logistic Regression**\n",
    "\n",
    "In this step, we train and evaluate a basic TF-IDF baseline model with logistic regression. Despite using a very small dataset, we observe a performance that is better than random. We will now check if BERT can outperform this strong baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBTyTh8Ui2D3"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(X_train)\n",
    "Xtest = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMjmweu6MIQU"
   },
   "source": [
    "We train a logistic regression model from scikit-learn on the newspaper training data, and then we use the trained model to make predictions on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R92S7JZfjiaC"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000).fit(Xtrain, y_train)\n",
    "predictions = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6YmW7WZMhh3"
   },
   "source": [
    "We can leverage the `classification_report` function provided by scikit-learn to assess the performance of the logistic regression model in terms of its ability to predict newspaper topics that match the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeJd8ogKjpg0",
    "outputId": "49cf0362-0be1-4923-b16c-a095e75f30dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      2783\n",
      "        True       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.98      2843\n",
      "   macro avg       0.49      0.50      0.49      2843\n",
      "weighted avg       0.96      0.98      0.97      2843\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuSduv8SsVS3"
   },
   "source": [
    "What do you think of this model? Not too bad for a baseline model, right? Lets see whether we can improve this using BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aow3FPpppZVE"
   },
   "source": [
    "## Encode data for BERT\n",
    "\n",
    "To prepare our data for use with BERT, we need to encode the texts and labels in a way that the model can understand. Here are the steps we'll follow:\n",
    "\n",
    "1. Convert the labels from strings to integers.\n",
    "\n",
    "2. Tokenize the texts, which involves breaking them up into individual words, and then convert the words into \"word pieces\" that can be matched with their corresponding embedding vectors.\n",
    "\n",
    "3. Truncate texts that are longer than 512 tokens, or pad texts that are shorter than 512 tokens with a special padding token.\n",
    "\n",
    "4. Add special tokens to the beginning and end of each document, including a start token, a separator between sentences, and a padding token as necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRs0dEIoUZtV"
   },
   "source": [
    "We will be using the `AutoTokenizer.from_pretrained()` module from HuggingFace library to encode our texts. This module will handle all the encoding for us, including breaking word tokens into word pieces, truncating to 512 tokens, and adding padding and special BERT tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BEvRqpGVMUD"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj8X7B30UvSj"
   },
   "source": [
    "In this section, we will generate a mapping of our news topics to integer keys. We begin by extracting the unique labels from our dataset and create a dictionary that associates each label with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSuo8gktjsVR"
   },
   "outputs": [],
   "source": [
    "unique_labels = set(label for label in y_train)\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_iAWMtBpfhj",
    "outputId": "ffd0e526-90d4-42da-8030-41b0015354da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([False, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vle8EgkelwRa",
    "outputId": "7bae75df-5334-4f57-ac33-8f5810a8f1fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EraNFBC8VnPu"
   },
   "source": [
    "Now let's encode our texts and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDuGq_n4pgZX"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings  = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "train_labels_encoded = [label2id[y] for y in y_train]\n",
    "val_labels_encoded = [label2id[y] for y in y_val]\n",
    "test_labels_encoded  = [label2id[y] for y in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X1sYEGsWDDh"
   },
   "source": [
    "**Examine a news article in the training set after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "4A89SN_ppiUP",
    "outputId": "47ad0a49-d65e-4c71-f7b4-b32ea8e37fb7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] Gers ##dorf : Ob ##rona sad ##ów bu ##dzi pod ##zi ##w demo ##krat ##ycznego s ##wia ##ta \" \" \" Prag ##ne pod ##ziek ##owa ##c wszystkim , którym nie jest ob ##oje ##tny stan polskiego sad ##own ##ictwa . ( . . . ) D ##ziek ##uje W ##am wszystkim , Was ##za posta ##wa jest wa ##zna i bu ##dzi sz ##ac ##unek nie tylko mó ##j i sed ##zió ##w , ale cal ##ego demo ##krat ##ycznego s ##wia ##ta \" \" - nap ##isal ##a w li ##sci ##e za ##mies ##zczony ##m na'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hvOn9RGWUMm"
   },
   "source": [
    "**Examine a news article in test set after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "OafZQFKSwG9E",
    "outputId": "a34fe456-d5b6-4436-9f90-29e7c4d794b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] W stanie woj ##enn ##ym za ##bron ##ili mu gra ##c , ter ##az do ##pad ##la go ' dobra zmian ##a ' . W ##y ##bit ##ny aktor z ##wo ##lni ##ony D ##zis dyrektor Polskiego Ce ##zar ##y Mora ##wski z ##wo ##lni ##l kolejne ##go cz ##lon ##ka zes ##pol ##u . Pa ##dlo na Andrzeja Wi ##lka , w ##y ##bit ##nego aktor ##a , w stanie woj ##enn ##ym op ##oz ##y ##c ##joni ##ste po ##z ##ba ##wione ##go prawa do w ##yk ##ony ##wania za ##wodu , który we w ##roc ##law\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jmt15FvW8FR"
   },
   "source": [
    "**Examine the training labels after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciemdVYwwMNz",
    "outputId": "4d70da89-843f-4fb8-bd8e-f73e8ba7629e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK1Ngb0wXBz9"
   },
   "source": [
    "**Examine the test labels after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TowwulYQwOff",
    "outputId": "f5c7ae31-88dc-4e76-fba7-c53607618f4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChcEv01TXI7v"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Create a custom Torch dataset by following these steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxWcyj0LXVtY"
   },
   "source": [
    "Here we combine the encoded labels and texts into dataset objects. We use the custom Torch `MyDataSet` class to make a `train_dataset` object from  the `train_encodings` and `train_labels_encoded`. We also make a `val_dataset`, `test_dataset` object from `test_encodings` and `val_encodings`, and `val_labels_encoded` and `test_labels_encoded`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4VCU-nepnqF"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyCFH3XEi4Ng"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
    "val_dataset = MyDataset(val_encodings, val_labels_encoded)\n",
    "test_dataset = MyDataset(test_encodings, test_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSoXOmDYYyAK"
   },
   "source": [
    "**Examine a news article in the Torch `training_dataset` after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "DbJerEgC1Qpc",
    "outputId": "cc36bc83-9fa2-44de-db6d-da0f3b7e8b36"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] Gers ##dorf : Ob ##rona sad ##ów bu ##dzi pod ##zi ##w demo ##krat ##ycznego s ##wia ##ta \" \" \" Prag ##ne pod ##ziek ##owa ##c wszystkim , którym nie jest ob ##oje ##tny stan polskiego sad ##own ##ictwa . ( . . . ) D ##ziek ##uje W ##am wszystkim , Was ##za posta ##wa jest wa ##zna i bu ##dzi sz ##ac ##unek nie tylko mó ##j i sed ##zió ##w , ale cal ##ego demo ##krat ##ycznego s ##wia ##ta \" \" - nap ##isal ##a w li ##sci ##e za ##mies ##zczony ##m na'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_dataset.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq1M2Et4Y3LB"
   },
   "source": [
    "**Examine a news article in the Torch `test_dataset` after encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "z65jnjVJ1aVB",
    "outputId": "4a0465e2-8233-4c36-9986-044590228ce8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] Nieuwe informa ##teur Za ##lm pak ##t fi ##dge ##t spin ##ner Bu ##ma af - De Sp ##eld Een grote tegen ##slag voor Sy ##brand Bu ##ma . De leider van het CD ##A moet zijn fi ##dge ##t spin ##ner in ##lever ##en . Volgens de nieuwe informa ##teur Gerrit Za ##lm lei ##dt het speelt ##je te veel af van de onder ##handeling ##en . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test_dataset.encodings[1].tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuNihFYcsVTL",
    "outputId": "5a4d9473-17bd-440e-bd9b-cd03dbdbfbf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkVgFcbCqKSu"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Initialize the pre-trained BERT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2pSuFUVaDhP"
   },
   "source": [
    "We load a pre-trained Dutch BERT model and transfer it to CUDA for efficient computation.\n",
    "\n",
    "**Note**: If you intend to repeat the fine-tuning process after previously executing the subsequent cells, ensure that you re-run this cell to reload the original pre-trained model before commencing the fine-tuning again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7k75REXp7UJ",
    "outputId": "59631ad2-28b6-4749-ad5a-3a30f85cfde4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label)).to(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRQZuqcAqQNI"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Configure the parameters required for fine-tuning BERT**\n",
    "\n",
    "The following parameters are crucial for fine-tuning BERT and will be specified in the HuggingFace TrainingArguments objects that we will subsequently pass to the HuggingFace Trainer object. While there are numerous other arguments, we'll focus on the fundamental ones and some common pitfalls.\n",
    "\n",
    "When fine-tuning your own model, it's critical to experiment with these parameters to identify the optimal configuration for your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYOaH9AhbCD_"
   },
   "source": [
    "| Parameter                     | Explanation                                                                                                                          |\n",
    "|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `num_train_epochs`            | The total number of training epochs. This refers to how many times the entire dataset will be processed. Too many epochs can lead to overfitting.|\n",
    "| `per_device_train_batch_size` | The batch size per device during training.                                                                                           |\n",
    "| `per_device_eval_batch_size`  | The batch size for evaluation.                                                                                                      |\n",
    "| `warmup_steps`                | The number of warmup steps for the learning rate scheduler. A smaller value is recommended for small datasets.                         |\n",
    "| `weight_decay`                | The strength of weight decay, which reduces the size of weights, similar to regularization.                                          |\n",
    "| `output_dir`                  | The directory where the fine-tuned model and configuration files will be saved.                                                     |\n",
    "| `logging_dir`                 | The directory where logs will be stored.                                                                                            |\n",
    "| `logging_steps`               | How often to print logging output. This enables us to terminate training early if the loss is not decreasing.                        |\n",
    "| `evaluation_strategy`         | Evaluates while training so that we can monitor accuracy improvements.                                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pb3xtidn-HJ"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Fine-tune the BERT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_SN_oGLV8Vw"
   },
   "source": [
    "Initially, we define a custom evaluation function that returns the accuracy of the model. However, this function can be modified to return other metrics such as precision, recall, F1 score, or any other desired evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNIt7fcnqUCp"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro', sample_weight=compute_sample_weight('balanced', labels))\n",
    "    return {'accuracy': acc, 'macro_f1': macro_f1, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xl5QLmWsAw"
   },
   "source": [
    "Then we create a HuggingFace `Trainer` object using the `TrainingArguments` object that we created above. We also send our `compute_metrics` function to the `Trainer` object, along with our test and train datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W25Ee80tsVTT"
   },
   "source": [
    "## **optimize your model based on a metric you select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZfu_oz0sVTT"
   },
   "outputs": [],
   "source": [
    "# macro_f1 would be good if we had multiple categories\n",
    "# but we only care about F1 for the true class (politics)\n",
    "metric_name = 'f1' # you can chance this for `accuracy` etc, according to the function `compute_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h2BrAPSsVTU"
   },
   "outputs": [],
   "source": [
    "# Instantiate an object of the TrainingArguments class with the following parameters:\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # Number of training epochs\n",
    "    num_train_epochs=5,\n",
    "    \n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=8,\n",
    "    \n",
    "    # Batch size for evaluation\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    # Learning rate for optimization\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    # Load the best model at the end of training\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Metric used for selecting the best model\n",
    "    metric_for_best_model=metric_name,\n",
    "    \n",
    "    # Number of warmup steps for the optimizer\n",
    "    warmup_steps=0,\n",
    "    \n",
    "    # L2 regularization weight decay\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Directory to save the fine-tuned model and configuration files\n",
    "    output_dir='./results',\n",
    "    \n",
    "    # Directory to store logs\n",
    "    logging_dir='./logs',\n",
    "    \n",
    "    # Log results every n steps\n",
    "    logging_steps=20,\n",
    "    \n",
    "    # Strategy for evaluating the model during training\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgc8FS50qV0_"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
    "    compute_metrics=compute_metrics      # our custom evaluation function \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo5QVLYjXGCN"
   },
   "source": [
    "Time to finally fine-tune! \n",
    "\n",
    "Be patient; if you've set everything in Colab to use GPUs, then it should only take a minute or two to run, but if you're running on CPU, it can take hours.\n",
    "\n",
    "After every 20 steps (as we specified in the TrainingArguments object), the trainer will output the current state of the model, including the training loss, validation loss, and accuracy (from our `compute_metrics` function).\n",
    "\n",
    "You should see the loss going down and the accuracy going up. If instead they are staying the same or oscillating, you probably need to change the fine-tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W64JwriVqcmk",
    "outputId": "6d2302e0-998d-499f-be6c-d24d5c94336b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3292' max='3995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3292/3995 1:22:23 < 17:36, 0.67 it/s, Epoch 4.12/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.128239</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.123054</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.114277</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.108925</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.099907</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.113196</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.129502</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.124402</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.099213</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.117173</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.120188</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.108932</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.114766</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.107412</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.110574</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.114394</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.132315</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.111664</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.110487</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.103096</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.105607</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.125066</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.082489</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.067759</td>\n",
       "      <td>0.983568</td>\n",
       "      <td>0.764119</td>\n",
       "      <td>0.578313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.117597</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.118157</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.113295</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.088607</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.066994</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.121276</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.112534</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.109850</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.109370</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.098074</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.073883</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.083218</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.122425</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.113416</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.111788</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.107774</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.106064</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.110902</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.114389</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.119757</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.112358</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.106622</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.115785</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.105568</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.128712</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.122599</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.122454</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.108591</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.109518</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.106499</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.106535</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.192731</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.105963</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.126931</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.116116</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.086124</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.078908</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.073872</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.072801</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.076040</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.077744</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.077287</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.079771</td>\n",
       "      <td>0.986854</td>\n",
       "      <td>0.696497</td>\n",
       "      <td>0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.075775</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.085635</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.084373</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.082819</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.081864</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.080616</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.080507</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.080322</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.082516</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.119176</td>\n",
       "      <td>0.980751</td>\n",
       "      <td>0.428816</td>\n",
       "      <td>0.163265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.115253</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.123138</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.382772</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.124427</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.382772</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.127444</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.121772</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.118110</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.119364</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.111790</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.101915</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.111323</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.117481</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.110996</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.110271</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.109413</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.109140</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.113606</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.113987</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.109069</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.103114</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.105438</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.105086</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.109880</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.108153</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.112192</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.111273</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.107838</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.104806</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.101326</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.104374</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.106886</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.107326</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.103056</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.109923</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.104974</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.106280</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.110337</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.112862</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.111613</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.107609</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.105852</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.107016</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.109268</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.113192</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>0.113578</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.108514</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.107932</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.111902</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.114185</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.113342</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.114328</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.116115</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.115746</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.118095</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.120808</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.077200</td>\n",
       "      <td>0.120084</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.116174</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.114051</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.114188</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.112756</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.113769</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.108137</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.108533</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.109258</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.110459</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.110474</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.109438</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='3995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  81/3995 01:38 < 1:21:11, 0.80 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.106735</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.103974</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.124315</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='466' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 01:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXeIZ_LFqeps"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Save fine-tuned model**\n",
    "\n",
    "The following cell will save the model and its configuration files to a directory in Colab. To preserve this model for future use, you should download the model to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxkDWDfvqeAo"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epiLftYkZrzc"
   },
   "source": [
    "(Optional) If you've already fine-tuned and saved the model, you can reload it using the following line. You don't have to run fine-tuning every time you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A54QySLrO5I"
   },
   "outputs": [],
   "source": [
    "# trainer = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpzV4hFsLmZ6"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Evaluate fine-tuned model on the validation set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IvfhrBtYYcz"
   },
   "source": [
    "The following function of the `Trainer` object will run the built-in evaluation, including our `compute_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dshtTH0WLtM1"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjpWjHKksVTf"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Evaluate fine-tuned model on the test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILJGLcCjYhPt"
   },
   "source": [
    "We may desire a more detailed evaluation of the model, hence we extract the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_E8oVjeLuv2"
   },
   "outputs": [],
   "source": [
    "predicted_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUYGfzczOuJE"
   },
   "outputs": [],
   "source": [
    "predicted_results.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqUTa5irLyN8"
   },
   "outputs": [],
   "source": [
    "predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
    "predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2jtqnJbPMpu"
   },
   "outputs": [],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVcMU45fLzli"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, \n",
    "                            predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddk-iKTQF-K3"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "## **Extracting Correct and Incorrect Classifications for Analysis**\n",
    "\n",
    "Now that we have obtained the predicted labels, let's perform some analysis.\n",
    "\n",
    "The fine-tuning and extraction of predicted labels using BERT is now complete. You can use the predicted labels just like you would with any other classification model. Here are some examples.\n",
    "\n",
    "To start, let's print out some example predictions that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7KrvGuZkDAV"
   },
   "outputs": [],
   "source": [
    "for _true_label, _predicted_label, _text in random.sample(list(zip(y_test, predicted_labels, X_test)), 20):\n",
    "  if _true_label == _predicted_label:\n",
    "    print('LABEL:', _true_label)\n",
    "    print('REVIEW TEXT:', _text[:100], '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW30Z6ynkDPI"
   },
   "source": [
    "Now let's print out some misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xmx1RSKDkIpG"
   },
   "outputs": [],
   "source": [
    "for _true_label, _predicted_label, _text in random.sample(list(zip(y_test, predicted_labels, X_test)), 80):\n",
    "  if _true_label != _predicted_label:\n",
    "    print('TRUE LABEL:', _true_label)\n",
    "    print('PREDICTED LABEL:', _predicted_label)\n",
    "    print('REVIEW TEXT:', _text[:100], '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MZqyFrckJBB"
   },
   "source": [
    "Finally, let's create some heatmaps to examine misclassification patterns. We could use these patterns to think about similarities and differences between genres, according to book reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSAgS6tvivvz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the number of classifications for each genre pair\n",
    "genre_classifications = Counter(zip(y_test, predicted_labels))\n",
    "\n",
    "# Convert the counts to a DataFrame and pivot to wide format\n",
    "df_wide = pd.DataFrame(genre_classifications, index=['Number of Classifications']).T.reset_index()\n",
    "df_wide.columns = ['True Genre', 'Predicted Genre', 'Number of Classifications']\n",
    "df_wide = df_wide.pivot_table(index='True Genre', columns='Predicted Genre', values='Number of Classifications', fill_value=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "sns.heatmap(df_wide, linewidths=1, cmap='Purples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiukJdYRqyjm"
   },
   "source": [
    "Looks good! We can see that overall, our model is assigning the correct labels for each genre. \n",
    "\n",
    "Now, let's remove the diagonal from the plot to highlight the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7GJQfYxi3ET"
   },
   "outputs": [],
   "source": [
    "genre_classifications_dict = defaultdict(int)\n",
    "for _true_label, _predicted_label in zip(y_test, predicted_labels):\n",
    "  if _true_label != _predicted_label: # Remove the diagonal to highlight misclassifications\n",
    "    genre_classifications_dict[(_true_label, _predicted_label)] += 1\n",
    "  \n",
    "dicts_to_plot = []\n",
    "for (_true_genre, _predicted_genre), _count in genre_classifications_dict.items():\n",
    "  dicts_to_plot.append({'True Genre': _true_genre,\n",
    "                        'Predicted Genre': _predicted_genre,\n",
    "                        'Number of Classifications': _count})\n",
    "  \n",
    "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "df_wide = df_to_plot.pivot_table(index='True Genre', \n",
    "                                 columns='Predicted Genre', \n",
    "                                 values='Number of Classifications')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "sns.heatmap(df_wide, linewidths=1, cmap='Purples')    \n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b0d7544f82776c2b902af54887e7cde1aa7d2da4fd982551ffc3948bf7522f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
